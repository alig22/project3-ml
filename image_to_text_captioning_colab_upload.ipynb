{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069d3afa",
   "metadata": {},
   "source": [
    "# üß† Mini-projet : Entra√Æner un mod√®le Image-to-Text dans Google Colab\n",
    "\n",
    "Ce notebook vous permet de :\n",
    "- Charger un mini-dataset image + texte\n",
    "- Visualiser les images et l√©gendes\n",
    "- Entra√Æner un mod√®le ViT + GPT-2\n",
    "- G√©n√©rer une l√©gende automatiquement √† partir d'une image\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 1 : Installer les biblioth√®ques n√©cessaires\n",
    "!pip install transformers datasets torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f19dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 2 : T√©l√©charger manuellement le fichier ZIP\n",
    "from google.colab import files\n",
    "\n",
    "print(\"‚¨ÜÔ∏è Veuillez t√©l√©verser le fichier 'dataset_caption_demo.zip'\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"dataset_caption_demo.zip\"\n",
    "extract_path = \"dataset_caption_demo\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"‚úÖ Fichiers extraits :\", os.listdir(extract_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e708d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 3 : Charger et afficher les donn√©es\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(f\"{extract_path}/captions.csv\")\n",
    "print(df.head())\n",
    "\n",
    "# Affichage d'une image avec l√©gende\n",
    "img_path = f\"{extract_path}/{df.iloc[0]['image_path']}\"\n",
    "caption = df.iloc[0]['caption']\n",
    "img = Image.open(img_path)\n",
    "plt.imshow(img)\n",
    "plt.title(caption)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45be6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 4 : Pr√©parer le mod√®le et les donn√©es\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Initialiser mod√®le et tokenizer\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\"google/vit-base-patch16-224\", \"gpt2\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Dataset personnalis√©\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, df, transform, tokenizer, base_path):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join(self.base_path, self.df.iloc[idx]['image_path'])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        labels = self.tokenizer(caption, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\").input_ids.squeeze()\n",
    "        return image, labels\n",
    "\n",
    "# Transformations pour les images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = CaptionDataset(df, transform, tokenizer, extract_path)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 5 : Entra√Æner bri√®vement le mod√®le (1 epoch)\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(1):\n",
    "    loop = tqdm(loader, desc=\"Entra√Ænement\")\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(pixel_values=images, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974cdb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 6 : G√©n√©rer une l√©gende pour une image\n",
    "model.eval()\n",
    "\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = feature_extractor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    output_ids = model.generate(pixel_values, max_length=64)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Exemple\n",
    "test_image = f\"{extract_path}/{df.iloc[1]['image_path']}\"\n",
    "print(\"üì∏ Image test :\", test_image)\n",
    "Image.open(test_image).show()\n",
    "print(\"üìù L√©gende g√©n√©r√©e :\", generate_caption(test_image))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}